Cross‑Lingual Summarization Attack

Overview

This repository contains code, scripts and documentation for a short research project exploring cross‑lingual summarization (CLS) as a watermark removal attack against large language models (LLMs).  The work was prepared on a very tight schedule as a submission for the NeurIPS Lock‑LLM workshop, which solicits contributions on watermarking, verification and robustness for generative models.  Our goal is to show that generating a summary in another language can significantly degrade the detectability of current watermarking schemes and to propose a simple mitigation strategy.

Recent work has demonstrated that watermark signals do not always survive translation: the cross‑lingual watermark removal attack (CWRA) translates a prompt to a pivot language and back to erase the watermark, dropping detection AUCs to random‑guessing levels ￼.  However, no prior study has looked at cross‑lingual summarization as the attack mechanism.  Summarization combines paraphrasing and translation, compressing the original text while changing its language, which may further dilute the “green‑list” token bias used by many watermarking algorithms.  This project therefore investigates CLS as a new black‑box attack vector and benchmarks it against translation and paraphrasing baselines.

Objectives

The main objectives of this project are:
	•	Implement a CLS attack pipeline that takes a watermarked English summary generated by an LLM and produces a summary in another language using off‑the‑shelf multilingual summarization models (mT5 or mBART).  Optionally, the foreign‑language summary is translated back into English to evaluate detection in the original language.
	•	Benchmark watermark robustness against three families of watermarking algorithms—KGW (green‑list based), Unigram (UW) and Semantic‑Invariant Robust (SIR)—using the MarkLLM toolkit ￼.  Compare CLS with paraphrasing and translation baselines.
	•	Measure quality preservation of the CLS outputs using standard metrics such as ROUGE‑1/2/L and length ratios to ensure the summaries remain faithful and fluent.
	•	Propose a simple defence, inspired by X‑SIR ￼, that ensembles detection scores on the foreign summary and its back‑translation to restore some detectability.
	•	Document findings and suggestions for future research, including potential variations such as code‑switching, style‑controlled summaries or multi‑hop attacks.

Novelty and Contribution

While previous studies have examined translation‑based attacks on watermarks, this project is (to our knowledge) the first to systematically investigate cross‑lingual summarization as a watermark removal channel.  Our novelty arises from:
	•	Combining compression and translation: CLS shortens and paraphrases the original response while changing its language.  This dual effect may reduce green‑token density more than translation alone, thereby lowering detection scores.
	•	Leveraging large‑scale cross‑lingual datasets: We use CrossSum ￼ and XL‑Sum ￼, which provide millions of article–summary pairs across 44–1500+ language pairs, enabling evaluation on both high‑ and low‑resource languages.
	•	Integrating open‑source toolkits: The experiments are built on MarkLLM, an extensible toolkit that implements many watermarking algorithms and detectors ￼.  Using existing APIs allows us to perform a meaningful study within three days without re‑implementing watermark algorithms.
	•	Providing a defence: We adapt the X‑SIR idea of semantic clustering across languages ￼ to test whether an ensemble of detection scores (on the foreign summary and its back‑translation) can partially recover detection performance.

Datasets
	•	XL‑Sum ￼: A multilingual abstractive summarization dataset containing about 1 million professionally annotated article–summary pairs in 44 languages.  It covers low‑, mid‑ and high‑resource languages and is used to fine‑tune mT5 models.
	•	CrossSum ￼: A large‑scale cross‑lingual summarization dataset with 1.68 million article–summary pairs across 1,500+ language pairs.  It was created by aligning articles in different languages and includes an embedding‑based evaluation metric (LaSE).  CrossSum provides pairs where the article and summary are in different languages.

Models

We use two open‑source multilingual sequence‑to‑sequence models as summarization engines:
	•	mT5 (multilingual T5): A multilingual transformer model pre‑trained on data from many languages.  We fine‑tune csebuetnlp/mT5_multilingual_XLSum on the XL‑Sum dataset for general summarization.
	•	mBART‑50: A many‑to‑many sequence‑to‑sequence model trained on multilingual translation and summarization tasks.  We use facebook/mbart-large-50-many-to-many-mmt for additional CLS experiments.

For watermarking, we rely on MarkLLM ￼ and its implementations of:
	•	KGW: A “green‑list” watermark that partitions the vocabulary into green (preferred) and red (disallowed) token sets, adding a bias to green tokens during generation and detecting via the proportion of green tokens ￼.
	•	Unigram (UW): An unbiased variant that adjusts logits but preserves the expected distribution of tokens ￼.
	•	SIR (Semantic‑Invariant Robust): Assigns similar adjustments to semantically similar prefixes, improving robustness against paraphrasing attacks ￼.
	•	X‑SIR (used for defence): Extends SIR by clustering semantically similar tokens across languages to maintain watermark signals after translation ￼.

Methodology
	1.	Watermark generation:
	•	Choose a base LLM (e.g. Llama‑3‑8B or a smaller open‑source model) and embed watermarks using KGW, UW and SIR via MarkLLM.
	•	For each article from XL‑Sum, generate a watermarked English summary.  Record the detection scores and quality metrics.
	2.	Baseline attacks:
	•	Paraphrase: Pass the watermarked summary through a monolingual paraphrasing model (e.g. Vamsi/T5-Paraphrase), then assess detection and ROUGE.
	•	Translation (CWRA): Translate the watermarked summary to a pivot language and back to English.  Use MarkLLM to evaluate detection, replicating the known translation attack baseline ￼.
	3.	Cross‑lingual summarization attack:
	•	Feed the watermarked English summary into the fine‑tuned mT5 or mBART‑50 model to produce a summary in a target language (e.g. Chinese, Spanish, Arabic).  Optionally, translate this summary back to English using a translation model to evaluate detection in the original language.
	•	Compute detection scores for KGW, UW and SIR on the target‑language summary and the back‑translation.
	•	Measure summarization quality with ROUGE against reference summaries and track the length ratio (compression level).
	4.	Defence evaluation:
	•	Implement an ensemble detector: for each CLS output, compute detection scores both in the target language and in its back‑translation, then take the maximum or average.  Compare this to SIR/X‑SIR and evaluate AUC and true‑positive rate improvements.
	5.	Ablation studies:
	•	Language variation: Perform experiments on multiple target languages (high‑resource vs. low‑resource) to assess resource‑dependency.
	•	Compression ratio: Generate summaries of different lengths (e.g. 20 %, 50 %, 80 % of the original summary) to test how compression affects detection.
	•	Decoding strategies: Compare beam search vs. stochastic sampling in the summarization models.
	6.	Analysis and reporting:
	•	Compile results into tables and plots (e.g., bar charts of AUC drop across attacks, scatter plots of summary length vs. detection score).
	•	Discuss the trade‑off between summary quality and watermark robustness, and identify which watermark schemes are most vulnerable to CLS.
	•	Summarize findings and propose directions for improving cross‑lingual watermark robustness.
