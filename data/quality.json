{
  "KGW_amharic": {
    "file": "data/KGW_amharic_clsa.json",
    "algorithm": "KGW",
    "language": "amharic",
    "quality": {
      "backtranslation": {
        "sacrebleu": {
          "score": 0.03109072627399487,
          "sys_len": 19599,
          "ref_len": 129214
        },
        "rouge": {
          "rouge1": {
            "p": 0.4697724614068722,
            "r": 0.09085391717943697,
            "f1": 0.14384062018520136
          },
          "rouge2": {
            "p": 0.11679597728447809,
            "r": 0.02615344199147806,
            "f1": 0.04005185759904141
          },
          "rougeL": {
            "p": 0.3426815343478876,
            "r": 0.06405742654421195,
            "f1": 0.10140150640832929
          }
        }
      },
      "paraphrase": {
        "sacrebleu": {
          "score": 20.527501044349762,
          "sys_len": 57349,
          "ref_len": 128958
        },
        "rouge": {
          "rouge1": {
            "p": 0.8356172487776552,
            "r": 0.43864987125416505,
            "f1": 0.5262662170732604
          },
          "rouge2": {
            "p": 0.7128156041903146,
            "r": 0.3995997301575057,
            "f1": 0.47214244979786935
          },
          "rougeL": {
            "p": 0.7525367678173217,
            "r": 0.41511777669893113,
            "f1": 0.4918454883319127
          }
        }
      },
      "xlsum": {
        "sacrebleu": {
          "score": 0.006719609235281497,
          "sys_len": 15934,
          "ref_len": 129214
        },
        "rouge": {
          "rouge1": {
            "p": 0.3287035970068045,
            "r": 0.056660093476368516,
            "f1": 0.08737338280047907
          },
          "rouge2": {
            "p": 0.10748429420893467,
            "r": 0.02106416043303236,
            "f1": 0.03256781651112824
          },
          "rougeL": {
            "p": 0.2562268416835274,
            "r": 0.037305324804371494,
            "f1": 0.057778048594625736
          }
        }
      }
    },
    "detection_metrics_clsa": {
      "sample_sizes": {
        "n_test": 300,
        "n_pos_test": 150,
        "n_neg_test": 150,
        "n_val": 200,
        "n_pos_val": 100,
        "n_neg_val": 100
      },
      "threshold": -0.07018624063435965,
      "threshold_source": "validation",
      "auroc@test": 0.5040666666666667,
      "auprc@test": 0.5243857255065036,
      "accuracy@thr": 0.5133333333333333,
      "precision@thr": 0.52,
      "recall@thr": 0.3466666666666667,
      "f1@thr": 0.416,
      "eer@test": 0.52,
      "tpr@fpr=0.010@test": 0.006666666666666667,
      "selection_details": {
        "eer_sel": 0.55,
        "thr_eer_sel": -0.6900655593423543
      }
    },
    "detection_metrics_back": {
      "sample_sizes": {
        "n_test": 300,
        "n_pos_test": 150,
        "n_neg_test": 150,
        "n_val": 200,
        "n_pos_val": 100,
        "n_neg_val": 100
      },
      "threshold": -1.2247448713915892,
      "threshold_source": "validation",
      "auroc@test": 0.5421777777777778,
      "auprc@test": 0.5305765535968056,
      "accuracy@thr": 0.5166666666666667,
      "precision@thr": 0.5094339622641509,
      "recall@thr": 0.9,
      "f1@thr": 0.6506024096385542,
      "eer@test": 0.45666666666666667,
      "tpr@fpr=0.010@test": 0.0,
      "selection_details": {
        "eer_sel": 0.48,
        "thr_eer_sel": 0.12803687993289598
      }
    }
  },
  "KGW_chinese": {
    "file": "data/KGW_chinese_clsa.json",
    "algorithm": "KGW",
    "language": "chinese",
    "quality": {
      "backtranslation": {
        "sacrebleu": {
          "score": 0.10088074128996126,
          "sys_len": 22508,
          "ref_len": 129214
        },
        "rouge": {
          "rouge1": {
            "p": 0.6178213265063288,
            "r": 0.1259747978908117,
            "f1": 0.20108345767892769
          },
          "rouge2": {
            "p": 0.20614238642995866,
            "r": 0.04114237162842842,
            "f1": 0.06588671752412605
          },
          "rougeL": {
            "p": 0.41699664732057345,
            "r": 0.08350470279574694,
            "f1": 0.13363764342914808
          }
        }
      },
      "paraphrase": {
        "sacrebleu": {
          "score": 20.085825436218496,
          "sys_len": 57990,
          "ref_len": 129214
        },
        "rouge": {
          "rouge1": {
            "p": 0.8238291506785081,
            "r": 0.4309359323798873,
            "f1": 0.5168338361199244
          },
          "rouge2": {
            "p": 0.6986554437228505,
            "r": 0.38935521988278676,
            "f1": 0.45970059493497206
          },
          "rougeL": {
            "p": 0.7382644750243776,
            "r": 0.40576100652213504,
            "f1": 0.4805917153449756
          }
        }
      },
      "xlsum": {
        "sacrebleu": {
          "score": 0.012642256252993011,
          "sys_len": 16477,
          "ref_len": 129214
        },
        "rouge": {
          "rouge1": {
            "p": 0.4713466952699193,
            "r": 0.07969228056865743,
            "f1": 0.12609052518018232
          },
          "rouge2": {
            "p": 0.15376364091827818,
            "r": 0.02832886828019775,
            "f1": 0.044805219954958245
          },
          "rougeL": {
            "p": 0.3439502996477407,
            "r": 0.05172283010114903,
            "f1": 0.08208936667583872
          }
        }
      }
    },
    "detection_metrics_clsa": {
      "sample_sizes": {
        "n_test": 300,
        "n_pos_test": 150,
        "n_neg_test": 150,
        "n_val": 200,
        "n_pos_val": 100,
        "n_neg_val": 100
      },
      "threshold": 0.43133109281375365,
      "threshold_source": "validation",
      "auroc@test": 0.503,
      "auprc@test": 0.5044639659554162,
      "accuracy@thr": 0.5233333333333333,
      "precision@thr": 0.5346534653465347,
      "recall@thr": 0.36,
      "f1@thr": 0.4302788844621514,
      "eer@test": 0.4666666666666667,
      "tpr@fpr=0.010@test": 0.0,
      "selection_details": {
        "eer_sel": 0.495,
        "thr_eer_sel": 0.13245323570650439
      }
    },
    "detection_metrics_back": {
      "sample_sizes": {
        "n_test": 300,
        "n_pos_test": 150,
        "n_neg_test": 150,
        "n_val": 200,
        "n_pos_val": 100,
        "n_neg_val": 100
      },
      "threshold": 0.24618298195866545,
      "threshold_source": "validation",
      "auroc@test": 0.4914222222222222,
      "auprc@test": 0.49614280912090425,
      "accuracy@thr": 0.47333333333333333,
      "precision@thr": 0.4649122807017544,
      "recall@thr": 0.35333333333333333,
      "f1@thr": 0.4015151515151515,
      "eer@test": 0.5133333333333334,
      "tpr@fpr=0.010@test": 0.006666666666666667,
      "selection_details": {
        "eer_sel": 0.5,
        "thr_eer_sel": -0.14586499149789456
      }
    }
  },
  "KGW_hindi": {
    "file": "data/KGW_hindi_clsa.json",
    "algorithm": "KGW",
    "language": "hindi",
    "quality": {
      "backtranslation": {
        "sacrebleu": {
          "score": 0.026411008663178772,
          "sys_len": 18074,
          "ref_len": 129214
        },
        "rouge": {
          "rouge1": {
            "p": 0.6406687828066068,
            "r": 0.10989874425917942,
            "f1": 0.18095485786776452
          },
          "rouge2": {
            "p": 0.2132075059158707,
            "r": 0.03656511955531515,
            "f1": 0.060230763709286164
          },
          "rougeL": {
            "p": 0.4331445518388241,
            "r": 0.07281018645267186,
            "f1": 0.12014376088028417
          }
        }
      },
      "paraphrase": {
        "sacrebleu": {
          "score": 21.01811014280016,
          "sys_len": 58682,
          "ref_len": 129214
        },
        "rouge": {
          "rouge1": {
            "p": 0.8327119309200932,
            "r": 0.44146357160445354,
            "f1": 0.5261648951620929
          },
          "rouge2": {
            "p": 0.7139909629117874,
            "r": 0.40164012595600324,
            "f1": 0.4722968223558311
          },
          "rougeL": {
            "p": 0.748985690930934,
            "r": 0.41704348723664186,
            "f1": 0.49128128359952855
          }
        }
      },
      "xlsum": {
        "sacrebleu": {
          "score": 0.05313766770143425,
          "sys_len": 21878,
          "ref_len": 129214
        },
        "rouge": {
          "rouge1": {
            "p": 0.36745172677372623,
            "r": 0.06808933383989739,
            "f1": 0.10756089346557178
          },
          "rouge2": {
            "p": 0.12395754204473891,
            "r": 0.024290045653286515,
            "f1": 0.03848095538416368
          },
          "rougeL": {
            "p": 0.2580248072629708,
            "r": 0.04343335590071648,
            "f1": 0.06874779394470842
          }
        }
      }
    },
    "detection_metrics_clsa": {
      "sample_sizes": {
        "n_test": 300,
        "n_pos_test": 150,
        "n_neg_test": 150,
        "n_val": 200,
        "n_pos_val": 100,
        "n_neg_val": 100
      },
      "threshold": 0.0,
      "threshold_source": "validation",
      "auroc@test": 0.5462666666666667,
      "auprc@test": 0.5656764662897553,
      "accuracy@thr": 0.5333333333333333,
      "precision@thr": 0.5294117647058824,
      "recall@thr": 0.6,
      "f1@thr": 0.5625,
      "eer@test": 0.45,
      "tpr@fpr=0.010@test": 0.0,
      "selection_details": {
        "eer_sel": 0.45999999999999996,
        "thr_eer_sel": 0.1203858530857692
      }
    },
    "detection_metrics_back": {
      "sample_sizes": {
        "n_test": 300,
        "n_pos_test": 150,
        "n_neg_test": 150,
        "n_val": 200,
        "n_pos_val": 100,
        "n_neg_val": 100
      },
      "threshold": 0.1690308509457033,
      "threshold_source": "validation",
      "auroc@test": 0.4861111111111111,
      "auprc@test": 0.4966619893089969,
      "accuracy@thr": 0.49,
      "precision@thr": 0.48739495798319327,
      "recall@thr": 0.38666666666666666,
      "f1@thr": 0.4312267657992565,
      "eer@test": 0.5233333333333333,
      "tpr@fpr=0.010@test": 0.006666666666666667,
      "selection_details": {
        "eer_sel": 0.43499999999999994,
        "thr_eer_sel": 0.11867816581938533
      }
    }
  },
  "KGW_spanish": {
    "file": "data/KGW_spanish_clsa.json",
    "algorithm": "KGW",
    "language": "spanish",
    "quality": {
      "backtranslation": {
        "sacrebleu": {
          "score": 0.06098725430948338,
          "sys_len": 20054,
          "ref_len": 129214
        },
        "rouge": {
          "rouge1": {
            "p": 0.6297455733766185,
            "r": 0.11921057732456082,
            "f1": 0.19318322426380452
          },
          "rouge2": {
            "p": 0.21775499118689814,
            "r": 0.04136496901235049,
            "f1": 0.06700317534304884
          },
          "rougeL": {
            "p": 0.43210382979137335,
            "r": 0.08006388510893864,
            "f1": 0.13008398246190536
          }
        }
      },
      "paraphrase": {
        "sacrebleu": {
          "score": 20.610273640234574,
          "sys_len": 57356,
          "ref_len": 129214
        },
        "rouge": {
          "rouge1": {
            "p": 0.8399655727514123,
            "r": 0.4419966192060881,
            "f1": 0.5284768720117322
          },
          "rouge2": {
            "p": 0.7206273138595082,
            "r": 0.40363642856590504,
            "f1": 0.475434382869838
          },
          "rougeL": {
            "p": 0.7529194731073082,
            "r": 0.41667540927278834,
            "f1": 0.49206348293536384
          }
        }
      },
      "xlsum": {
        "sacrebleu": {
          "score": 0.1224939155475302,
          "sys_len": 24213,
          "ref_len": 129214
        },
        "rouge": {
          "rouge1": {
            "p": 0.41990164002466146,
            "r": 0.10445118499903402,
            "f1": 0.16223527898554832
          },
          "rouge2": {
            "p": 0.13967735510821294,
            "r": 0.034969465694812754,
            "f1": 0.05426322055203975
          },
          "rougeL": {
            "p": 0.27765297239209497,
            "r": 0.06831473456394267,
            "f1": 0.10622097482207417
          }
        }
      }
    },
    "detection_metrics_clsa": {
      "sample_sizes": {
        "n_test": 300,
        "n_pos_test": 150,
        "n_neg_test": 150,
        "n_val": 200,
        "n_pos_val": 100,
        "n_neg_val": 100
      },
      "threshold": -0.39056673294247163,
      "threshold_source": "validation",
      "auroc@test": 0.5844222222222222,
      "auprc@test": 0.5503170938852351,
      "accuracy@thr": 0.5566666666666666,
      "precision@thr": 0.5431472081218274,
      "recall@thr": 0.7133333333333334,
      "f1@thr": 0.6167146974063401,
      "eer@test": 0.43999999999999995,
      "tpr@fpr=0.010@test": 0.0,
      "selection_details": {
        "eer_sel": 0.495,
        "thr_eer_sel": 0.11867816581938533
      }
    },
    "detection_metrics_back": {
      "sample_sizes": {
        "n_test": 300,
        "n_pos_test": 150,
        "n_neg_test": 150,
        "n_val": 200,
        "n_pos_val": 100,
        "n_neg_val": 100
      },
      "threshold": -0.1643989873053573,
      "threshold_source": "validation",
      "auroc@test": 0.5112222222222222,
      "auprc@test": 0.51572396096636,
      "accuracy@thr": 0.49666666666666665,
      "precision@thr": 0.4972972972972973,
      "recall@thr": 0.6133333333333333,
      "f1@thr": 0.5492537313432836,
      "eer@test": 0.4766666666666667,
      "tpr@fpr=0.010@test": 0.0,
      "selection_details": {
        "eer_sel": 0.43999999999999995,
        "thr_eer_sel": 0.0
      }
    }
  },
  "KGW_swahili": {
    "file": "data/KGW_swahili_clsa.json",
    "algorithm": "KGW",
    "language": "swahili",
    "quality": {
      "backtranslation": {
        "sacrebleu": {
          "score": 0.029055871367920434,
          "sys_len": 19105,
          "ref_len": 129214
        },
        "rouge": {
          "rouge1": {
            "p": 0.5804216309538259,
            "r": 0.10468858689070236,
            "f1": 0.17174340997347542
          },
          "rouge2": {
            "p": 0.1670083164377851,
            "r": 0.029597350197955022,
            "f1": 0.04882414654988059
          },
          "rougeL": {
            "p": 0.3986677232846159,
            "r": 0.07033763501057642,
            "f1": 0.11580048440418653
          }
        }
      },
      "paraphrase": {
        "sacrebleu": {
          "score": 21.131340057292203,
          "sys_len": 58598,
          "ref_len": 129214
        },
        "rouge": {
          "rouge1": {
            "p": 0.8381774072671732,
            "r": 0.4404588419531543,
            "f1": 0.5283684150257646
          },
          "rouge2": {
            "p": 0.714263900922673,
            "r": 0.4012658095462772,
            "f1": 0.473852568669169
          },
          "rougeL": {
            "p": 0.7512264436887454,
            "r": 0.41556384286960407,
            "f1": 0.4921850283378436
          }
        }
      },
      "xlsum": {
        "sacrebleu": {
          "score": 0.016183655969198903,
          "sys_len": 20087,
          "ref_len": 129214
        },
        "rouge": {
          "rouge1": {
            "p": 0.179006325904107,
            "r": 0.037879245354147266,
            "f1": 0.060247097355649204
          },
          "rouge2": {
            "p": 0.052339234745096136,
            "r": 0.01092175112609214,
            "f1": 0.017442387514137278
          },
          "rougeL": {
            "p": 0.1299115500954638,
            "r": 0.026633463584880463,
            "f1": 0.042609618710890904
          }
        }
      }
    },
    "detection_metrics_clsa": {
      "sample_sizes": {
        "n_test": 300,
        "n_pos_test": 150,
        "n_neg_test": 150,
        "n_val": 200,
        "n_pos_val": 100,
        "n_neg_val": 100
      },
      "threshold": -0.254000254000381,
      "threshold_source": "validation",
      "auroc@test": 0.5202888888888889,
      "auprc@test": 0.5224591799271683,
      "accuracy@thr": 0.5,
      "precision@thr": 0.5,
      "recall@thr": 0.6066666666666667,
      "f1@thr": 0.5481927710843374,
      "eer@test": 0.4933333333333333,
      "tpr@fpr=0.010@test": 0.0,
      "selection_details": {
        "eer_sel": 0.52,
        "thr_eer_sel": -0.10482848367219183
      }
    },
    "detection_metrics_back": {
      "sample_sizes": {
        "n_test": 300,
        "n_pos_test": 150,
        "n_neg_test": 150,
        "n_val": 200,
        "n_pos_val": 100,
        "n_neg_val": 100
      },
      "threshold": 1.028991510855053,
      "threshold_source": "validation",
      "auroc@test": 0.5539777777777778,
      "auprc@test": 0.5619527218510019,
      "accuracy@thr": 0.5266666666666666,
      "precision@thr": 0.5869565217391305,
      "recall@thr": 0.18,
      "f1@thr": 0.2755102040816326,
      "eer@test": 0.4633333333333334,
      "tpr@fpr=0.010@test": 0.03333333333333333,
      "selection_details": {
        "eer_sel": 0.525,
        "thr_eer_sel": -0.1643989873053573
      }
    }
  },
  "SIR_amharic": {
    "file": "data/SIR_amharic_clsa.json",
    "algorithm": "SIR",
    "language": "amharic",
    "quality": {
      "backtranslation": {
        "sacrebleu": {
          "score": 0.02525758974596825,
          "sys_len": 20265,
          "ref_len": 134864
        },
        "rouge": {
          "rouge1": {
            "p": 0.4585179339558831,
            "r": 0.08366059544735194,
            "f1": 0.13505413460509672
          },
          "rouge2": {
            "p": 0.10972757316438812,
            "r": 0.023092494154270428,
            "f1": 0.03648453554436651
          },
          "rougeL": {
            "p": 0.33560105317639805,
            "r": 0.05831671247933633,
            "f1": 0.09475355088021746
          }
        }
      },
      "paraphrase": {
        "sacrebleu": {
          "score": 22.890491318232005,
          "sys_len": 61533,
          "ref_len": 134623
        },
        "rouge": {
          "rouge1": {
            "p": 0.8603062538348195,
            "r": 0.4640386564598218,
            "f1": 0.5494323551741221
          },
          "rouge2": {
            "p": 0.7446923898321315,
            "r": 0.4290385065380955,
            "f1": 0.5001876629794823
          },
          "rougeL": {
            "p": 0.7777567472216036,
            "r": 0.44085329744765095,
            "f1": 0.5156675148874075
          }
        }
      },
      "xlsum": {
        "sacrebleu": {
          "score": 0.004360653939416026,
          "sys_len": 16013,
          "ref_len": 134864
        },
        "rouge": {
          "rouge1": {
            "p": 0.32440386099812746,
            "r": 0.053947740843436766,
            "f1": 0.0838153571651014
          },
          "rouge2": {
            "p": 0.10369918367455948,
            "r": 0.019020360008615397,
            "f1": 0.029419328697892638
          },
          "rougeL": {
            "p": 0.25155211955997053,
            "r": 0.035340976968937336,
            "f1": 0.05511222073199393
          }
        }
      }
    },
    "detection_metrics_clsa": {
      "sample_sizes": {
        "n_test": 300,
        "n_pos_test": 150,
        "n_neg_test": 150,
        "n_val": 200,
        "n_pos_val": 100,
        "n_neg_val": 100
      },
      "threshold": -0.28835905521460203,
      "threshold_source": "validation",
      "auroc@test": 0.5275555555555556,
      "auprc@test": 0.518660544561633,
      "accuracy@thr": 0.4866666666666667,
      "precision@thr": 0.4925925925925926,
      "recall@thr": 0.8866666666666667,
      "f1@thr": 0.6333333333333333,
      "eer@test": 0.4733333333333334,
      "tpr@fpr=0.010@test": 0.006666666666666667,
      "selection_details": {
        "eer_sel": 0.52,
        "thr_eer_sel": -0.1878671860549508
      }
    },
    "detection_metrics_back": {
      "sample_sizes": {
        "n_test": 299,
        "n_pos_test": 150,
        "n_neg_test": 149,
        "n_val": 199,
        "n_pos_val": 99,
        "n_neg_val": 100
      },
      "threshold": -0.28205005022195667,
      "threshold_source": "validation",
      "auroc@test": 0.5394183445190157,
      "auprc@test": 0.535784358719902,
      "accuracy@thr": 0.5317725752508361,
      "precision@thr": 0.5201612903225806,
      "recall@thr": 0.86,
      "f1@thr": 0.6482412060301508,
      "eer@test": 0.47827740492170023,
      "tpr@fpr=0.010@test": 0.006666666666666667,
      "selection_details": {
        "eer_sel": 0.5025252525252525,
        "thr_eer_sel": -0.07142893224954605
      }
    }
  },
  "SIR_chinese": {
    "file": "data/SIR_chinese_clsa.json",
    "algorithm": "SIR",
    "language": "chinese",
    "quality": {
      "backtranslation": {
        "sacrebleu": {
          "score": 0.08954672642982309,
          "sys_len": 23033,
          "ref_len": 134864
        },
        "rouge": {
          "rouge1": {
            "p": 0.6006660724336318,
            "r": 0.11898288847917882,
            "f1": 0.19242056163085117
          },
          "rouge2": {
            "p": 0.19253399332691026,
            "r": 0.03736323800059741,
            "f1": 0.06063457450422036
          },
          "rougeL": {
            "p": 0.4031819136644411,
            "r": 0.07812266470394853,
            "f1": 0.12680084024810437
          }
        }
      },
      "paraphrase": {
        "sacrebleu": {
          "score": 22.826001289939065,
          "sys_len": 61546,
          "ref_len": 133989
        },
        "rouge": {
          "rouge1": {
            "p": 0.8530262218275221,
            "r": 0.4623225135790604,
            "f1": 0.5469950006206441
          },
          "rouge2": {
            "p": 0.7358495474619302,
            "r": 0.4263389085223706,
            "f1": 0.49656165028806626
          },
          "rougeL": {
            "p": 0.7675681769522255,
            "r": 0.43853072416735145,
            "f1": 0.512196621430186
          }
        }
      },
      "xlsum": {
        "sacrebleu": {
          "score": 0.006259838252316759,
          "sys_len": 15890,
          "ref_len": 134864
        },
        "rouge": {
          "rouge1": {
            "p": 0.4472774604918292,
            "r": 0.07000608004197964,
            "f1": 0.11178900822269879
          },
          "rouge2": {
            "p": 0.13529512379511213,
            "r": 0.02421497379060697,
            "f1": 0.03857848876958641
          },
          "rougeL": {
            "p": 0.3346381693041093,
            "r": 0.04531063222251353,
            "f1": 0.07276175926396715
          }
        }
      }
    },
    "detection_metrics_clsa": {
      "sample_sizes": {
        "n_test": 300,
        "n_pos_test": 150,
        "n_neg_test": 150,
        "n_val": 200,
        "n_pos_val": 100,
        "n_neg_val": 100
      },
      "threshold": 0.019036760835936577,
      "threshold_source": "validation",
      "auroc@test": 0.5314666666666668,
      "auprc@test": 0.5305169847991815,
      "accuracy@thr": 0.5133333333333333,
      "precision@thr": 0.5112359550561798,
      "recall@thr": 0.6066666666666667,
      "f1@thr": 0.5548780487804879,
      "eer@test": 0.5,
      "tpr@fpr=0.010@test": 0.0,
      "selection_details": {
        "eer_sel": 0.43000000000000005,
        "thr_eer_sel": 0.0664067856873138
      }
    },
    "detection_metrics_back": {
      "sample_sizes": {
        "n_test": 300,
        "n_pos_test": 150,
        "n_neg_test": 150,
        "n_val": 200,
        "n_pos_val": 100,
        "n_neg_val": 100
      },
      "threshold": 0.11165784239768982,
      "threshold_source": "validation",
      "auroc@test": 0.5558222222222222,
      "auprc@test": 0.5782428278118064,
      "accuracy@thr": 0.5366666666666666,
      "precision@thr": 0.5846153846153846,
      "recall@thr": 0.25333333333333335,
      "f1@thr": 0.35348837209302325,
      "eer@test": 0.4633333333333334,
      "tpr@fpr=0.010@test": 0.013333333333333334,
      "selection_details": {
        "eer_sel": 0.42000000000000004,
        "thr_eer_sel": 0.02896577850827631
      }
    }
  },
  "SIR_hindi": {
    "file": "data/SIR_hindi_clsa.json",
    "algorithm": "SIR",
    "language": "hindi",
    "quality": {
      "backtranslation": {
        "sacrebleu": {
          "score": 0.016363324081777757,
          "sys_len": 17617,
          "ref_len": 134864
        },
        "rouge": {
          "rouge1": {
            "p": 0.6338133672579953,
            "r": 0.09932569448629901,
            "f1": 0.16671221147633267
          },
          "rouge2": {
            "p": 0.20706216075281633,
            "r": 0.03285834931773541,
            "f1": 0.05509098417773209
          },
          "rougeL": {
            "p": 0.4330740511238553,
            "r": 0.06682497357346712,
            "f1": 0.11240880602171915
          }
        }
      },
      "paraphrase": {
        "sacrebleu": {
          "score": 21.64547590096412,
          "sys_len": 60677,
          "ref_len": 134797
        },
        "rouge": {
          "rouge1": {
            "p": 0.8558673207104442,
            "r": 0.4507648492738471,
            "f1": 0.5351546268459084
          },
          "rouge2": {
            "p": 0.7397057152026107,
            "r": 0.41494049093201363,
            "f1": 0.4855716645634206
          },
          "rougeL": {
            "p": 0.7738083898575941,
            "r": 0.42775246447641924,
            "f1": 0.5019699554156453
          }
        }
      },
      "xlsum": {
        "sacrebleu": {
          "score": 0.04117786363105713,
          "sys_len": 21754,
          "ref_len": 134864
        },
        "rouge": {
          "rouge1": {
            "p": 0.38438747211129415,
            "r": 0.05999124632489613,
            "f1": 0.09646122711279825
          },
          "rouge2": {
            "p": 0.12433893445821136,
            "r": 0.021488166821531634,
            "f1": 0.03444532922887823
          },
          "rougeL": {
            "p": 0.28222818563977675,
            "r": 0.039011445281076744,
            "f1": 0.06284418910025034
          }
        }
      }
    },
    "detection_metrics_clsa": {
      "sample_sizes": {
        "n_test": 300,
        "n_pos_test": 150,
        "n_neg_test": 150,
        "n_val": 200,
        "n_pos_val": 100,
        "n_neg_val": 100
      },
      "threshold": 0.0153261520141779,
      "threshold_source": "validation",
      "auroc@test": 0.5676444444444444,
      "auprc@test": 0.578366316182511,
      "accuracy@thr": 0.5333333333333333,
      "precision@thr": 0.526595744680851,
      "recall@thr": 0.66,
      "f1@thr": 0.5857988165680473,
      "eer@test": 0.44666666666666666,
      "tpr@fpr=0.010@test": 0.04,
      "selection_details": {
        "eer_sel": 0.43000000000000005,
        "thr_eer_sel": 0.05883848303701819
      }
    },
    "detection_metrics_back": {
      "sample_sizes": {
        "n_test": 300,
        "n_pos_test": 150,
        "n_neg_test": 150,
        "n_val": 200,
        "n_pos_val": 100,
        "n_neg_val": 100
      },
      "threshold": -0.007750039750879461,
      "threshold_source": "validation",
      "auroc@test": 0.5614222222222223,
      "auprc@test": 0.5623749096408062,
      "accuracy@thr": 0.5366666666666666,
      "precision@thr": 0.5419847328244275,
      "recall@thr": 0.47333333333333333,
      "f1@thr": 0.505338078291815,
      "eer@test": 0.45333333333333337,
      "tpr@fpr=0.010@test": 0.013333333333333334,
      "selection_details": {
        "eer_sel": 0.43500000000000005,
        "thr_eer_sel": -0.030303917147896507
      }
    }
  },
  "SIR_spanish": {
    "file": "data/SIR_spanish_clsa.json",
    "algorithm": "SIR",
    "language": "spanish",
    "quality": {
      "backtranslation": {
        "sacrebleu": {
          "score": 0.043865434880756664,
          "sys_len": 20033,
          "ref_len": 134864
        },
        "rouge": {
          "rouge1": {
            "p": 0.6245593721592592,
            "r": 0.10964140580291962,
            "f1": 0.1806081809320668
          },
          "rouge2": {
            "p": 0.21302593439238368,
            "r": 0.036825282372182216,
            "f1": 0.06087884843045717
          },
          "rougeL": {
            "p": 0.43283365928351925,
            "r": 0.07384260339641287,
            "f1": 0.12219654919365122
          }
        }
      },
      "paraphrase": {
        "sacrebleu": {
          "score": 22.339495161865912,
          "sys_len": 61162,
          "ref_len": 134864
        },
        "rouge": {
          "rouge1": {
            "p": 0.8606939183641803,
            "r": 0.46184195890964524,
            "f1": 0.5464096848587789
          },
          "rouge2": {
            "p": 0.7418065764220534,
            "r": 0.42677113241910275,
            "f1": 0.4968817939834411
          },
          "rougeL": {
            "p": 0.776482571494489,
            "r": 0.43862664130387063,
            "f1": 0.5124473459241806
          }
        }
      },
      "xlsum": {
        "sacrebleu": {
          "score": 0.0899339206016575,
          "sys_len": 24172,
          "ref_len": 134864
        },
        "rouge": {
          "rouge1": {
            "p": 0.3930393836482283,
            "r": 0.09453940086070473,
            "f1": 0.14816409156157792
          },
          "rouge2": {
            "p": 0.13027924754074977,
            "r": 0.031309563738434944,
            "f1": 0.049094709044758045
          },
          "rougeL": {
            "p": 0.26259162980834305,
            "r": 0.062128850392479666,
            "f1": 0.09764203931514691
          }
        }
      }
    },
    "detection_metrics_clsa": {
      "sample_sizes": {
        "n_test": 300,
        "n_pos_test": 150,
        "n_neg_test": 150,
        "n_val": 200,
        "n_pos_val": 100,
        "n_neg_val": 100
      },
      "threshold": 0.13991671428084373,
      "threshold_source": "validation",
      "auroc@test": 0.5774666666666667,
      "auprc@test": 0.60371449597472,
      "accuracy@thr": 0.5366666666666666,
      "precision@thr": 0.6666666666666666,
      "recall@thr": 0.14666666666666667,
      "f1@thr": 0.24043715846994534,
      "eer@test": 0.4066666666666666,
      "tpr@fpr=0.010@test": 0.06,
      "selection_details": {
        "eer_sel": 0.515,
        "thr_eer_sel": -0.051721621500818354
      }
    },
    "detection_metrics_back": {
      "sample_sizes": {
        "n_test": 300,
        "n_pos_test": 150,
        "n_neg_test": 150,
        "n_val": 200,
        "n_pos_val": 100,
        "n_neg_val": 100
      },
      "threshold": 0.08531041639115737,
      "threshold_source": "validation",
      "auroc@test": 0.5817333333333332,
      "auprc@test": 0.6050385647197147,
      "accuracy@thr": 0.5366666666666666,
      "precision@thr": 0.5797101449275363,
      "recall@thr": 0.26666666666666666,
      "f1@thr": 0.365296803652968,
      "eer@test": 0.43999999999999995,
      "tpr@fpr=0.010@test": 0.04,
      "selection_details": {
        "eer_sel": 0.47,
        "thr_eer_sel": -0.04149770598078883
      }
    }
  },
  "SIR_swahili": {
    "file": "data/SIR_swahili_clsa.json",
    "algorithm": "SIR",
    "language": "swahili",
    "quality": {
      "backtranslation": {
        "sacrebleu": {
          "score": 0.027170719005782072,
          "sys_len": 19565,
          "ref_len": 134864
        },
        "rouge": {
          "rouge1": {
            "p": 0.583468933390742,
            "r": 0.1019377225350131,
            "f1": 0.16852015226315215
          },
          "rouge2": {
            "p": 0.1709835576987725,
            "r": 0.03000708540050138,
            "f1": 0.04959466618804359
          },
          "rougeL": {
            "p": 0.4024502924400391,
            "r": 0.06901739470287838,
            "f1": 0.11438763811613824
          }
        }
      },
      "paraphrase": {
        "sacrebleu": {
          "score": 22.876084646749653,
          "sys_len": 61809,
          "ref_len": 134864
        },
        "rouge": {
          "rouge1": {
            "p": 0.8556474580440085,
            "r": 0.46457933556642006,
            "f1": 0.5478203506646547
          },
          "rouge2": {
            "p": 0.7423924868507153,
            "r": 0.4303004009360495,
            "f1": 0.4998439794899938
          },
          "rougeL": {
            "p": 0.7742176311605051,
            "r": 0.44235984821196717,
            "f1": 0.5154294376349959
          }
        }
      },
      "xlsum": {
        "sacrebleu": {
          "score": 0.015348359105978824,
          "sys_len": 20160,
          "ref_len": 134864
        },
        "rouge": {
          "rouge1": {
            "p": 0.1822170432225834,
            "r": 0.03661445991480949,
            "f1": 0.05888075057077021
          },
          "rouge2": {
            "p": 0.05877501269303108,
            "r": 0.012033519226728366,
            "f1": 0.019246235973327343
          },
          "rougeL": {
            "p": 0.13242926944987568,
            "r": 0.026120000553607442,
            "f1": 0.04212803628406241
          }
        }
      }
    },
    "detection_metrics_clsa": {
      "sample_sizes": {
        "n_test": 300,
        "n_pos_test": 150,
        "n_neg_test": 150,
        "n_val": 200,
        "n_pos_val": 100,
        "n_neg_val": 100
      },
      "threshold": 0.04790045247703302,
      "threshold_source": "validation",
      "auroc@test": 0.5444444444444445,
      "auprc@test": 0.5364077675195738,
      "accuracy@thr": 0.5366666666666666,
      "precision@thr": 0.5426356589147286,
      "recall@thr": 0.4666666666666667,
      "f1@thr": 0.5017921146953405,
      "eer@test": 0.43666666666666665,
      "tpr@fpr=0.010@test": 0.0,
      "selection_details": {
        "eer_sel": 0.44999999999999996,
        "thr_eer_sel": 0.028168758659651787
      }
    },
    "detection_metrics_back": {
      "sample_sizes": {
        "n_test": 300,
        "n_pos_test": 150,
        "n_neg_test": 150,
        "n_val": 200,
        "n_pos_val": 100,
        "n_neg_val": 100
      },
      "threshold": 0.07137004818235125,
      "threshold_source": "validation",
      "auroc@test": 0.4663999999999999,
      "auprc@test": 0.4923626646683139,
      "accuracy@thr": 0.48333333333333334,
      "precision@thr": 0.47572815533980584,
      "recall@thr": 0.32666666666666666,
      "f1@thr": 0.38735177865612647,
      "eer@test": 0.5166666666666666,
      "tpr@fpr=0.010@test": 0.006666666666666667,
      "selection_details": {
        "eer_sel": 0.41500000000000004,
        "thr_eer_sel": 0.01733686657328355
      }
    }
  },
  "Unigram_amharic": {
    "file": "data/Unigram_amharic_clsa.json",
    "algorithm": "Unigram",
    "language": "amharic",
    "quality": {
      "backtranslation": {
        "sacrebleu": {
          "score": 0.01388971317341465,
          "sys_len": 20151,
          "ref_len": 144448
        },
        "rouge": {
          "rouge1": {
            "p": 0.46097086180319297,
            "r": 0.07893439472232307,
            "f1": 0.12831753323078152
          },
          "rouge2": {
            "p": 0.10927677479545234,
            "r": 0.021027994396386637,
            "f1": 0.033751051017498604
          },
          "rougeL": {
            "p": 0.34346370597454035,
            "r": 0.056464726065089045,
            "f1": 0.09225663660676958
          }
        }
      },
      "paraphrase": {
        "sacrebleu": {
          "score": 14.51634716600086,
          "sys_len": 56578,
          "ref_len": 143844
        },
        "rouge": {
          "rouge1": {
            "p": 0.8235792840164959,
            "r": 0.35860295796966113,
            "f1": 0.45987118235150964
          },
          "rouge2": {
            "p": 0.6990403402410753,
            "r": 0.3202870595158293,
            "f1": 0.40539804097023224
          },
          "rougeL": {
            "p": 0.7373036952866082,
            "r": 0.3346715542024441,
            "f1": 0.4244972133277282
          }
        }
      },
      "xlsum": {
        "sacrebleu": {
          "score": 0.0013866802087696788,
          "sys_len": 15271,
          "ref_len": 144448
        },
        "rouge": {
          "rouge1": {
            "p": 0.28673061120267906,
            "r": 0.04490256079305983,
            "f1": 0.07010079527305499
          },
          "rouge2": {
            "p": 0.0998519356718953,
            "r": 0.01589277200662216,
            "f1": 0.024864329839306358
          },
          "rougeL": {
            "p": 0.2246691174198106,
            "r": 0.02955157158210795,
            "f1": 0.04640260292768654
          }
        }
      }
    },
    "detection_metrics_clsa": {
      "sample_sizes": {
        "n_test": 300,
        "n_pos_test": 150,
        "n_neg_test": 150,
        "n_val": 200,
        "n_pos_val": 100,
        "n_neg_val": 100
      },
      "threshold": 1.6035674514745464,
      "threshold_source": "validation",
      "auroc@test": 0.5548222222222222,
      "auprc@test": 0.5494873061812994,
      "accuracy@thr": 0.5366666666666666,
      "precision@thr": 0.526829268292683,
      "recall@thr": 0.72,
      "f1@thr": 0.6084507042253521,
      "eer@test": 0.4766666666666667,
      "tpr@fpr=0.010@test": 0.0,
      "selection_details": {
        "eer_sel": 0.44999999999999996,
        "thr_eer_sel": 4.245349044041859
      }
    },
    "detection_metrics_back": {
      "sample_sizes": {
        "n_test": 300,
        "n_pos_test": 150,
        "n_neg_test": 150,
        "n_val": 200,
        "n_pos_val": 100,
        "n_neg_val": 100
      },
      "threshold": 1.5666989036012806,
      "threshold_source": "validation",
      "auroc@test": 0.5908444444444445,
      "auprc@test": 0.6000924969974781,
      "accuracy@thr": 0.5433333333333333,
      "precision@thr": 0.7096774193548387,
      "recall@thr": 0.14666666666666667,
      "f1@thr": 0.2430939226519337,
      "eer@test": 0.42333333333333334,
      "tpr@fpr=0.010@test": 0.02,
      "selection_details": {
        "eer_sel": 0.5,
        "thr_eer_sel": 0.0
      }
    }
  },
  "Unigram_chinese": {
    "file": "data/Unigram_chinese_clsa.json",
    "algorithm": "Unigram",
    "language": "chinese",
    "quality": {
      "backtranslation": {
        "sacrebleu": {
          "score": 0.06958263864905079,
          "sys_len": 23838,
          "ref_len": 144448
        },
        "rouge": {
          "rouge1": {
            "p": 0.6052074847316438,
            "r": 0.11324508210414216,
            "f1": 0.1842875077137289
          },
          "rouge2": {
            "p": 0.19849734853564033,
            "r": 0.03583493546818658,
            "f1": 0.05884031269932933
          },
          "rougeL": {
            "p": 0.421885507437454,
            "r": 0.07745118916591401,
            "f1": 0.12638228850435196
          }
        }
      },
      "paraphrase": {
        "sacrebleu": {
          "score": 14.557763483916304,
          "sys_len": 56568,
          "ref_len": 144448
        },
        "rouge": {
          "rouge1": {
            "p": 0.8328563837196077,
            "r": 0.36570509574761056,
            "f1": 0.4659601703579977
          },
          "rouge2": {
            "p": 0.7024270716397797,
            "r": 0.3254289984033215,
            "f1": 0.4089009149319577
          },
          "rougeL": {
            "p": 0.7399250383892424,
            "r": 0.3404122607238929,
            "f1": 0.42864922402094535
          }
        }
      },
      "xlsum": {
        "sacrebleu": {
          "score": 0.0038923564315004318,
          "sys_len": 16085,
          "ref_len": 144448
        },
        "rouge": {
          "rouge1": {
            "p": 0.4622781267441686,
            "r": 0.06800938448332307,
            "f1": 0.1099762068772452
          },
          "rouge2": {
            "p": 0.1530661759668473,
            "r": 0.02391334401139237,
            "f1": 0.03861378741187758
          },
          "rougeL": {
            "p": 0.34943011633947857,
            "r": 0.04524289996545277,
            "f1": 0.07354758581245023
          }
        }
      }
    },
    "detection_metrics_clsa": {
      "sample_sizes": {
        "n_test": 300,
        "n_pos_test": 150,
        "n_neg_test": 150,
        "n_val": 200,
        "n_pos_val": 100,
        "n_neg_val": 100
      },
      "threshold": -1.4985372985307104,
      "threshold_source": "validation",
      "auroc@test": 0.5549999999999999,
      "auprc@test": 0.5537823767042356,
      "accuracy@thr": 0.51,
      "precision@thr": 0.5062240663900415,
      "recall@thr": 0.8133333333333334,
      "f1@thr": 0.6240409207161125,
      "eer@test": 0.4633333333333334,
      "tpr@fpr=0.010@test": 0.006666666666666667,
      "selection_details": {
        "eer_sel": 0.51,
        "thr_eer_sel": -0.5547001962252291
      }
    },
    "detection_metrics_back": {
      "sample_sizes": {
        "n_test": 300,
        "n_pos_test": 150,
        "n_neg_test": 150,
        "n_val": 200,
        "n_pos_val": 100,
        "n_neg_val": 100
      },
      "threshold": 0.13483997249264842,
      "threshold_source": "validation",
      "auroc@test": 0.6081333333333333,
      "auprc@test": 0.6401817831339944,
      "accuracy@thr": 0.5666666666666667,
      "precision@thr": 0.5568181818181818,
      "recall@thr": 0.6533333333333333,
      "f1@thr": 0.6012269938650306,
      "eer@test": 0.42000000000000004,
      "tpr@fpr=0.010@test": 0.06666666666666667,
      "selection_details": {
        "eer_sel": 0.395,
        "thr_eer_sel": 0.15617376188860607
      }
    }
  },
  "Unigram_hindi": {
    "file": "data/Unigram_hindi_clsa.json",
    "algorithm": "Unigram",
    "language": "hindi",
    "quality": {
      "backtranslation": {
        "sacrebleu": {
          "score": 0.011156517369522521,
          "sys_len": 18185,
          "ref_len": 144448
        },
        "rouge": {
          "rouge1": {
            "p": 0.6229358349171963,
            "r": 0.09453068310842967,
            "f1": 0.15873305857862866
          },
          "rouge2": {
            "p": 0.2017917388550401,
            "r": 0.031246635461161777,
            "f1": 0.05226732136900244
          },
          "rougeL": {
            "p": 0.42995809203587865,
            "r": 0.06339516562050165,
            "f1": 0.10685560612046952
          }
        }
      },
      "paraphrase": {
        "sacrebleu": {
          "score": 13.923701412526642,
          "sys_len": 55249,
          "ref_len": 144448
        },
        "rouge": {
          "rouge1": {
            "p": 0.8405522492771665,
            "r": 0.3616138583578043,
            "f1": 0.4647320010191851
          },
          "rouge2": {
            "p": 0.7084517196533505,
            "r": 0.3233111931091611,
            "f1": 0.4092101025195607
          },
          "rougeL": {
            "p": 0.7476894926680936,
            "r": 0.3363910454282291,
            "f1": 0.4273261593246142
          }
        }
      },
      "xlsum": {
        "sacrebleu": {
          "score": 0.024700345001593193,
          "sys_len": 21916,
          "ref_len": 144448
        },
        "rouge": {
          "rouge1": {
            "p": 0.3731899741733701,
            "r": 0.057229993374219565,
            "f1": 0.09209035633416818
          },
          "rouge2": {
            "p": 0.11618756036023739,
            "r": 0.019938585599203435,
            "f1": 0.032037555083154146
          },
          "rougeL": {
            "p": 0.2764686258653444,
            "r": 0.03710435916767013,
            "f1": 0.05995237499719745
          }
        }
      }
    },
    "detection_metrics_clsa": {
      "sample_sizes": {
        "n_test": 300,
        "n_pos_test": 150,
        "n_neg_test": 150,
        "n_val": 200,
        "n_pos_val": 100,
        "n_neg_val": 100
      },
      "threshold": -4.503024286273868,
      "threshold_source": "validation",
      "auroc@test": 0.4169555555555555,
      "auprc@test": 0.46659999808920816,
      "accuracy@thr": 0.43333333333333335,
      "precision@thr": 0.4484536082474227,
      "recall@thr": 0.58,
      "f1@thr": 0.5058139534883721,
      "eer@test": 0.5966666666666667,
      "tpr@fpr=0.010@test": 0.006666666666666667,
      "selection_details": {
        "eer_sel": 0.49,
        "thr_eer_sel": -4.005180976482571
      }
    },
    "detection_metrics_back": {
      "sample_sizes": {
        "n_test": 300,
        "n_pos_test": 150,
        "n_neg_test": 150,
        "n_val": 200,
        "n_pos_val": 100,
        "n_neg_val": 100
      },
      "threshold": -0.15249857033260467,
      "threshold_source": "validation",
      "auroc@test": 0.6074888888888887,
      "auprc@test": 0.5944492810561387,
      "accuracy@thr": 0.5833333333333334,
      "precision@thr": 0.5786163522012578,
      "recall@thr": 0.6133333333333333,
      "f1@thr": 0.5954692556634305,
      "eer@test": 0.41333333333333333,
      "tpr@fpr=0.010@test": 0.02,
      "selection_details": {
        "eer_sel": 0.405,
        "thr_eer_sel": 0.0
      }
    }
  },
  "Unigram_spanish": {
    "file": "data/Unigram_spanish_clsa.json",
    "algorithm": "Unigram",
    "language": "spanish",
    "quality": {
      "backtranslation": {
        "sacrebleu": {
          "score": 0.030422921468227854,
          "sys_len": 20229,
          "ref_len": 144448
        },
        "rouge": {
          "rouge1": {
            "p": 0.633677130200018,
            "r": 0.10419343774703851,
            "f1": 0.1734641687469011
          },
          "rouge2": {
            "p": 0.2278436294575021,
            "r": 0.03714241118037501,
            "f1": 0.06196390524437844
          },
          "rougeL": {
            "p": 0.44386458306542226,
            "r": 0.07111927778722124,
            "f1": 0.11887915298740874
          }
        }
      },
      "paraphrase": {
        "sacrebleu": {
          "score": 14.18185597889194,
          "sys_len": 56049,
          "ref_len": 144028
        },
        "rouge": {
          "rouge1": {
            "p": 0.826457608861448,
            "r": 0.36069104190164514,
            "f1": 0.46261100071412475
          },
          "rouge2": {
            "p": 0.6966157209012054,
            "r": 0.3191464291137521,
            "f1": 0.40366163940131666
          },
          "rougeL": {
            "p": 0.7344526204803136,
            "r": 0.33382752142178646,
            "f1": 0.42345430193464145
          }
        }
      },
      "xlsum": {
        "sacrebleu": {
          "score": 0.06891615927994349,
          "sys_len": 24344,
          "ref_len": 144448
        },
        "rouge": {
          "rouge1": {
            "p": 0.40982407103702845,
            "r": 0.09103102999538894,
            "f1": 0.14485353159478506
          },
          "rouge2": {
            "p": 0.13919093683674155,
            "r": 0.031005937096415376,
            "f1": 0.04934533402117782
          },
          "rougeL": {
            "p": 0.27421004709202645,
            "r": 0.05990750126357258,
            "f1": 0.09560556095927704
          }
        }
      }
    },
    "detection_metrics_clsa": {
      "sample_sizes": {
        "n_test": 300,
        "n_pos_test": 150,
        "n_neg_test": 150,
        "n_val": 200,
        "n_pos_val": 100,
        "n_neg_val": 100
      },
      "threshold": 0.14002800840280097,
      "threshold_source": "validation",
      "auroc@test": 0.6736222222222222,
      "auprc@test": 0.6667445784177654,
      "accuracy@thr": 0.6333333333333333,
      "precision@thr": 0.6234567901234568,
      "recall@thr": 0.6733333333333333,
      "f1@thr": 0.6474358974358975,
      "eer@test": 0.3766666666666667,
      "tpr@fpr=0.010@test": 0.02666666666666667,
      "selection_details": {
        "eer_sel": 0.395,
        "thr_eer_sel": 0.12803687993289598
      }
    },
    "detection_metrics_back": {
      "sample_sizes": {
        "n_test": 300,
        "n_pos_test": 150,
        "n_neg_test": 150,
        "n_val": 200,
        "n_pos_val": 100,
        "n_neg_val": 100
      },
      "threshold": -0.17407765595569785,
      "threshold_source": "validation",
      "auroc@test": 0.6473111111111112,
      "auprc@test": 0.6143888431406814,
      "accuracy@thr": 0.61,
      "precision@thr": 0.5854922279792746,
      "recall@thr": 0.7533333333333333,
      "f1@thr": 0.6588921282798834,
      "eer@test": 0.4033333333333333,
      "tpr@fpr=0.010@test": 0.013333333333333334,
      "selection_details": {
        "eer_sel": 0.42500000000000004,
        "thr_eer_sel": 0.0
      }
    }
  },
  "Unigram_swahili": {
    "file": "data/Unigram_swahili_clsa.json",
    "algorithm": "Unigram",
    "language": "swahili",
    "quality": {
      "backtranslation": {
        "sacrebleu": {
          "score": 0.019911561877717966,
          "sys_len": 19984,
          "ref_len": 144448
        },
        "rouge": {
          "rouge1": {
            "p": 0.5891134395508191,
            "r": 0.09519385549316929,
            "f1": 0.15925980530758002
          },
          "rouge2": {
            "p": 0.17891240390662577,
            "r": 0.02906638253851798,
            "f1": 0.04867266323643586
          },
          "rougeL": {
            "p": 0.415808084427111,
            "r": 0.06565941383334511,
            "f1": 0.11018497668780958
          }
        }
      },
      "paraphrase": {
        "sacrebleu": {
          "score": 14.138479800433778,
          "sys_len": 55490,
          "ref_len": 143715
        },
        "rouge": {
          "rouge1": {
            "p": 0.8358909442538193,
            "r": 0.361638330969815,
            "f1": 0.46438964191651344
          },
          "rouge2": {
            "p": 0.7034138133107684,
            "r": 0.3217849361611202,
            "f1": 0.4068499710100944
          },
          "rougeL": {
            "p": 0.7427615982803377,
            "r": 0.33603527504090536,
            "f1": 0.42635567467321384
          }
        }
      },
      "xlsum": {
        "sacrebleu": {
          "score": 0.010451727663855733,
          "sys_len": 20383,
          "ref_len": 144448
        },
        "rouge": {
          "rouge1": {
            "p": 0.18346046228033078,
            "r": 0.034837159517264835,
            "f1": 0.056770650242602694
          },
          "rouge2": {
            "p": 0.058179853051535876,
            "r": 0.010981122110445975,
            "f1": 0.01792360781618582
          },
          "rougeL": {
            "p": 0.1323226897899284,
            "r": 0.024433910766402864,
            "f1": 0.04002233102287402
          }
        }
      }
    },
    "detection_metrics_clsa": {
      "sample_sizes": {
        "n_test": 300,
        "n_pos_test": 150,
        "n_neg_test": 150,
        "n_val": 200,
        "n_pos_val": 100,
        "n_neg_val": 100
      },
      "threshold": -0.5423261445466404,
      "threshold_source": "validation",
      "auroc@test": 0.5840666666666666,
      "auprc@test": 0.5684137887064862,
      "accuracy@thr": 0.5333333333333333,
      "precision@thr": 0.5190839694656488,
      "recall@thr": 0.9066666666666666,
      "f1@thr": 0.6601941747572816,
      "eer@test": 0.44666666666666666,
      "tpr@fpr=0.010@test": 0.0,
      "selection_details": {
        "eer_sel": 0.475,
        "thr_eer_sel": 0.9761870601839528
      }
    },
    "detection_metrics_back": {
      "sample_sizes": {
        "n_test": 300,
        "n_pos_test": 150,
        "n_neg_test": 150,
        "n_val": 200,
        "n_pos_val": 100,
        "n_neg_val": 100
      },
      "threshold": -0.14907119849998599,
      "threshold_source": "validation",
      "auroc@test": 0.627488888888889,
      "auprc@test": 0.6131412036383704,
      "accuracy@thr": 0.58,
      "precision@thr": 0.572289156626506,
      "recall@thr": 0.6333333333333333,
      "f1@thr": 0.6012658227848101,
      "eer@test": 0.4166666666666667,
      "tpr@fpr=0.010@test": 0.006666666666666667,
      "selection_details": {
        "eer_sel": 0.43499999999999994,
        "thr_eer_sel": 0.16012815380508713
      }
    }
  },
  "XSIR_amharic": {
    "file": "data/XSIR_amharic_clsa.json",
    "algorithm": "XSIR",
    "language": "amharic",
    "quality": {
      "backtranslation": {
        "sacrebleu": {
          "score": 0.0138567220140856,
          "sys_len": 19627,
          "ref_len": 144207
        },
        "rouge": {
          "rouge1": {
            "p": 0.48040410225000624,
            "r": 0.0832636642986786,
            "f1": 0.1349337062098374
          },
          "rouge2": {
            "p": 0.12056751391604702,
            "r": 0.02358694634173736,
            "f1": 0.03763484880069866
          },
          "rougeL": {
            "p": 0.35041120589999764,
            "r": 0.057549237880716864,
            "f1": 0.09390754066025317
          }
        }
      },
      "paraphrase": {
        "sacrebleu": {
          "score": 17.274419240894645,
          "sys_len": 60204,
          "ref_len": 144207
        },
        "rouge": {
          "rouge1": {
            "p": 0.8257843273715676,
            "r": 0.38851539944374053,
            "f1": 0.4817367818931235
          },
          "rouge2": {
            "p": 0.7055761917432071,
            "r": 0.3516720538235064,
            "f1": 0.4296347241625187
          },
          "rougeL": {
            "p": 0.7410657215971838,
            "r": 0.3653988182449283,
            "f1": 0.44772839856388547
          }
        }
      },
      "xlsum": {
        "sacrebleu": {
          "score": 0.002116455314695556,
          "sys_len": 15673,
          "ref_len": 144207
        },
        "rouge": {
          "rouge1": {
            "p": 0.32243694304445075,
            "r": 0.05327857683246778,
            "f1": 0.08279661865620364
          },
          "rouge2": {
            "p": 0.11784795395651475,
            "r": 0.01920782596809861,
            "f1": 0.02992724582110096
          },
          "rougeL": {
            "p": 0.24936239651143902,
            "r": 0.0340834990006536,
            "f1": 0.0531958822522947
          }
        }
      }
    },
    "detection_metrics_clsa": {
      "sample_sizes": {
        "n_test": 300,
        "n_pos_test": 150,
        "n_neg_test": 150,
        "n_val": 200,
        "n_pos_val": 100,
        "n_neg_val": 100
      },
      "threshold": -0.02125161491310248,
      "threshold_source": "validation",
      "auroc@test": 0.4933777777777778,
      "auprc@test": 0.5103805644695272,
      "accuracy@thr": 0.5066666666666667,
      "precision@thr": 0.5131578947368421,
      "recall@thr": 0.26,
      "f1@thr": 0.34513274336283184,
      "eer@test": 0.5,
      "tpr@fpr=0.010@test": 0.013333333333333334,
      "selection_details": {
        "eer_sel": 0.475,
        "thr_eer_sel": -0.19592103167720462
      }
    },
    "detection_metrics_back": {
      "sample_sizes": {
        "n_test": 300,
        "n_pos_test": 150,
        "n_neg_test": 150,
        "n_val": 200,
        "n_pos_val": 100,
        "n_neg_val": 100
      },
      "threshold": 0.13782664991560437,
      "threshold_source": "validation",
      "auroc@test": 0.43257777777777784,
      "auprc@test": 0.4560616382922394,
      "accuracy@thr": 0.49,
      "precision@thr": 0.4634146341463415,
      "recall@thr": 0.12666666666666668,
      "f1@thr": 0.19895287958115182,
      "eer@test": 0.5566666666666666,
      "tpr@fpr=0.010@test": 0.0,
      "selection_details": {
        "eer_sel": 0.51,
        "thr_eer_sel": -0.09380459558704625
      }
    }
  },
  "XSIR_chinese": {
    "file": "data/XSIR_chinese_clsa.json",
    "algorithm": "XSIR",
    "language": "chinese",
    "quality": {
      "backtranslation": {
        "sacrebleu": {
          "score": 0.06764801847348235,
          "sys_len": 23488,
          "ref_len": 144207
        },
        "rouge": {
          "rouge1": {
            "p": 0.6103967610669313,
            "r": 0.11211032091627664,
            "f1": 0.18281048527101434
          },
          "rouge2": {
            "p": 0.20518502090716115,
            "r": 0.03616121316520101,
            "f1": 0.05945040178332348
          },
          "rougeL": {
            "p": 0.41868687369279306,
            "r": 0.07542461024731117,
            "f1": 0.12332331677137782
          }
        }
      },
      "paraphrase": {
        "sacrebleu": {
          "score": 16.83106127164047,
          "sys_len": 58710,
          "ref_len": 143376
        },
        "rouge": {
          "rouge1": {
            "p": 0.8347851105831607,
            "r": 0.39327422177182264,
            "f1": 0.4860805221324732
          },
          "rouge2": {
            "p": 0.7055552396505823,
            "r": 0.35380637555886235,
            "f1": 0.43046456327278837
          },
          "rougeL": {
            "p": 0.7436146896086215,
            "r": 0.3682778512149725,
            "f1": 0.4495526100069009
          }
        }
      },
      "xlsum": {
        "sacrebleu": {
          "score": 0.00789050489602727,
          "sys_len": 17242,
          "ref_len": 144207
        },
        "rouge": {
          "rouge1": {
            "p": 0.5026408504425303,
            "r": 0.07705066384191002,
            "f1": 0.12382290637020252
          },
          "rouge2": {
            "p": 0.16277687810048663,
            "r": 0.026876909361950127,
            "f1": 0.04345820946901189
          },
          "rougeL": {
            "p": 0.3793770199097626,
            "r": 0.05117848576762069,
            "f1": 0.08266424288436745
          }
        }
      }
    },
    "detection_metrics_clsa": {
      "sample_sizes": {
        "n_test": 300,
        "n_pos_test": 150,
        "n_neg_test": 150,
        "n_val": 200,
        "n_pos_val": 100,
        "n_neg_val": 100
      },
      "threshold": -0.11964474532201334,
      "threshold_source": "validation",
      "auroc@test": 0.5386666666666666,
      "auprc@test": 0.572272702528798,
      "accuracy@thr": 0.5133333333333333,
      "precision@thr": 0.51010101010101,
      "recall@thr": 0.6733333333333333,
      "f1@thr": 0.5804597701149425,
      "eer@test": 0.4766666666666667,
      "tpr@fpr=0.010@test": 0.02666666666666667,
      "selection_details": {
        "eer_sel": 0.505,
        "thr_eer_sel": -0.06060177286313514
      }
    },
    "detection_metrics_back": {
      "sample_sizes": {
        "n_test": 300,
        "n_pos_test": 150,
        "n_neg_test": 150,
        "n_val": 200,
        "n_pos_val": 100,
        "n_neg_val": 100
      },
      "threshold": -0.025291775330537704,
      "threshold_source": "validation",
      "auroc@test": 0.4643111111111111,
      "auprc@test": 0.48977045814881737,
      "accuracy@thr": 0.52,
      "precision@thr": 0.5340909090909091,
      "recall@thr": 0.31333333333333335,
      "f1@thr": 0.3949579831932773,
      "eer@test": 0.5133333333333333,
      "tpr@fpr=0.010@test": 0.0,
      "selection_details": {
        "eer_sel": 0.54,
        "thr_eer_sel": -0.11191371509007045
      }
    }
  },
  "XSIR_hindi": {
    "file": "data/XSIR_hindi_clsa.json",
    "algorithm": "XSIR",
    "language": "hindi",
    "quality": {
      "backtranslation": {
        "sacrebleu": {
          "score": 0.01401482477316006,
          "sys_len": 18515,
          "ref_len": 144207
        },
        "rouge": {
          "rouge1": {
            "p": 0.6389232030480554,
            "r": 0.0978326133479877,
            "f1": 0.1641403874192799
          },
          "rouge2": {
            "p": 0.20767919747704788,
            "r": 0.03140293428336018,
            "f1": 0.05277807920863703
          },
          "rougeL": {
            "p": 0.43950828674666115,
            "r": 0.06554449373954965,
            "f1": 0.11032283275074234
          }
        }
      },
      "paraphrase": {
        "sacrebleu": {
          "score": 16.790715265613663,
          "sys_len": 59115,
          "ref_len": 143469
        },
        "rouge": {
          "rouge1": {
            "p": 0.8269922408012299,
            "r": 0.38687758141217765,
            "f1": 0.48127536675621835
          },
          "rouge2": {
            "p": 0.7009350067758885,
            "r": 0.3492306536021902,
            "f1": 0.4273521711543097
          },
          "rougeL": {
            "p": 0.7421911488146729,
            "r": 0.3638345002338901,
            "f1": 0.4470049749306598
          }
        }
      },
      "xlsum": {
        "sacrebleu": {
          "score": 0.032052417039162645,
          "sys_len": 22061,
          "ref_len": 144207
        },
        "rouge": {
          "rouge1": {
            "p": 0.39447906172231645,
            "r": 0.06294006981964283,
            "f1": 0.1026788302311874
          },
          "rouge2": {
            "p": 0.1402503553186992,
            "r": 0.02193823300553186,
            "f1": 0.03605372037258564
          },
          "rougeL": {
            "p": 0.2856031870478262,
            "r": 0.04112788742367646,
            "f1": 0.06738887177583923
          }
        }
      }
    },
    "detection_metrics_clsa": {
      "sample_sizes": {
        "n_test": 300,
        "n_pos_test": 150,
        "n_neg_test": 150,
        "n_val": 200,
        "n_pos_val": 100,
        "n_neg_val": 100
      },
      "threshold": 0.19049296989662093,
      "threshold_source": "validation",
      "auroc@test": 0.5267111111111111,
      "auprc@test": 0.5556364505442567,
      "accuracy@thr": 0.5266666666666666,
      "precision@thr": 0.6818181818181818,
      "recall@thr": 0.1,
      "f1@thr": 0.1744186046511628,
      "eer@test": 0.48333333333333334,
      "tpr@fpr=0.010@test": 0.006666666666666667,
      "selection_details": {
        "eer_sel": 0.575,
        "thr_eer_sel": -0.08906371075873333
      }
    },
    "detection_metrics_back": {
      "sample_sizes": {
        "n_test": 300,
        "n_pos_test": 150,
        "n_neg_test": 150,
        "n_val": 200,
        "n_pos_val": 100,
        "n_neg_val": 100
      },
      "threshold": 0.01697250207265218,
      "threshold_source": "validation",
      "auroc@test": 0.4723111111111111,
      "auprc@test": 0.46692581378386366,
      "accuracy@thr": 0.49333333333333335,
      "precision@thr": 0.4880952380952381,
      "recall@thr": 0.2733333333333333,
      "f1@thr": 0.3504273504273504,
      "eer@test": 0.5233333333333333,
      "tpr@fpr=0.010@test": 0.0,
      "selection_details": {
        "eer_sel": 0.51,
        "thr_eer_sel": -0.048287201386231646
      }
    }
  },
  "XSIR_spanish": {
    "file": "data/XSIR_spanish_clsa.json",
    "algorithm": "XSIR",
    "language": "spanish",
    "quality": {
      "backtranslation": {
        "sacrebleu": {
          "score": 0.02867789992422002,
          "sys_len": 20024,
          "ref_len": 144207
        },
        "rouge": {
          "rouge1": {
            "p": 0.6357414757083516,
            "r": 0.10762163772099755,
            "f1": 0.17730629497542105
          },
          "rouge2": {
            "p": 0.2201862843103001,
            "r": 0.03600665876282425,
            "f1": 0.05973430820193019
          },
          "rougeL": {
            "p": 0.4417578790063013,
            "r": 0.07237965414735395,
            "f1": 0.11969226332329898
          }
        }
      },
      "paraphrase": {
        "sacrebleu": {
          "score": 17.620708636371994,
          "sys_len": 61020,
          "ref_len": 144207
        },
        "rouge": {
          "rouge1": {
            "p": 0.8245315527912175,
            "r": 0.3908680904176516,
            "f1": 0.483998456950739
          },
          "rouge2": {
            "p": 0.7014321531097869,
            "r": 0.3513558932530655,
            "f1": 0.42879954395213127
          },
          "rougeL": {
            "p": 0.7421556861752435,
            "r": 0.36688188422119766,
            "f1": 0.44928255983493737
          }
        }
      },
      "xlsum": {
        "sacrebleu": {
          "score": 0.0661248932445943,
          "sys_len": 24206,
          "ref_len": 144207
        },
        "rouge": {
          "rouge1": {
            "p": 0.4141029379681837,
            "r": 0.0940049940767402,
            "f1": 0.1486924303091453
          },
          "rouge2": {
            "p": 0.13718487174408625,
            "r": 0.030588450578727173,
            "f1": 0.0485934670898052
          },
          "rougeL": {
            "p": 0.27157151279867114,
            "r": 0.06034276129697869,
            "f1": 0.09578506354409014
          }
        }
      }
    },
    "detection_metrics_clsa": {
      "sample_sizes": {
        "n_test": 300,
        "n_pos_test": 150,
        "n_neg_test": 150,
        "n_val": 200,
        "n_pos_val": 100,
        "n_neg_val": 100
      },
      "threshold": 0.20501009877337967,
      "threshold_source": "validation",
      "auroc@test": 0.5095111111111111,
      "auprc@test": 0.49388702693927544,
      "accuracy@thr": 0.4866666666666667,
      "precision@thr": 0.25,
      "recall@thr": 0.013333333333333334,
      "f1@thr": 0.02531645569620253,
      "eer@test": 0.48,
      "tpr@fpr=0.010@test": 0.0,
      "selection_details": {
        "eer_sel": 0.49,
        "thr_eer_sel": -0.05390043665723103
      }
    },
    "detection_metrics_back": {
      "sample_sizes": {
        "n_test": 300,
        "n_pos_test": 150,
        "n_neg_test": 150,
        "n_val": 200,
        "n_pos_val": 100,
        "n_neg_val": 100
      },
      "threshold": -0.5407506473686384,
      "threshold_source": "validation",
      "auroc@test": 0.4435111111111111,
      "auprc@test": 0.4700154302409139,
      "accuracy@thr": 0.5033333333333333,
      "precision@thr": 0.5016835016835017,
      "recall@thr": 0.9933333333333333,
      "f1@thr": 0.6666666666666666,
      "eer@test": 0.5233333333333333,
      "tpr@fpr=0.010@test": 0.0,
      "selection_details": {
        "eer_sel": 0.52,
        "thr_eer_sel": -0.06170308003400234
      }
    }
  },
  "XSIR_swahili": {
    "file": "data/XSIR_swahili_clsa.json",
    "algorithm": "XSIR",
    "language": "swahili",
    "quality": {
      "backtranslation": {
        "sacrebleu": {
          "score": 0.01568784946256218,
          "sys_len": 19435,
          "ref_len": 144207
        },
        "rouge": {
          "rouge1": {
            "p": 0.5812821882060941,
            "r": 0.09601259414174351,
            "f1": 0.1593250925542961
          },
          "rouge2": {
            "p": 0.17203293565763647,
            "r": 0.02786055370085907,
            "f1": 0.046409374212594376
          },
          "rougeL": {
            "p": 0.40249039056297437,
            "r": 0.06423335470102674,
            "f1": 0.10715402175588466
          }
        }
      },
      "paraphrase": {
        "sacrebleu": {
          "score": 16.954343495945945,
          "sys_len": 59556,
          "ref_len": 143874
        },
        "rouge": {
          "rouge1": {
            "p": 0.8272153565284387,
            "r": 0.388616563580986,
            "f1": 0.4832372486521416
          },
          "rouge2": {
            "p": 0.699573367299806,
            "r": 0.3510507242018087,
            "f1": 0.4291920206332246
          },
          "rougeL": {
            "p": 0.7392625044717348,
            "r": 0.36516615805259794,
            "f1": 0.4482975066390602
          }
        }
      },
      "xlsum": {
        "sacrebleu": {
          "score": 0.010724744943955315,
          "sys_len": 20457,
          "ref_len": 144207
        },
        "rouge": {
          "rouge1": {
            "p": 0.20288194628858192,
            "r": 0.04044541210449894,
            "f1": 0.06505870266074092
          },
          "rouge2": {
            "p": 0.06214014954535243,
            "r": 0.012130050538565513,
            "f1": 0.019609641450956325
          },
          "rougeL": {
            "p": 0.14377426594844492,
            "r": 0.02774704226884651,
            "f1": 0.04489292867251136
          }
        }
      }
    },
    "detection_metrics_clsa": {
      "sample_sizes": {
        "n_test": 300,
        "n_pos_test": 150,
        "n_neg_test": 150,
        "n_val": 200,
        "n_pos_val": 100,
        "n_neg_val": 100
      },
      "threshold": 0.2758450578061902,
      "threshold_source": "validation",
      "auroc@test": 0.46768888888888893,
      "auprc@test": 0.4916406174876237,
      "accuracy@thr": 0.48333333333333334,
      "precision@thr": 0.38095238095238093,
      "recall@thr": 0.05333333333333334,
      "f1@thr": 0.0935672514619883,
      "eer@test": 0.5133333333333333,
      "tpr@fpr=0.010@test": 0.013333333333333334,
      "selection_details": {
        "eer_sel": 0.52,
        "thr_eer_sel": 0.09813696945109117
      }
    },
    "detection_metrics_back": {
      "sample_sizes": {
        "n_test": 300,
        "n_pos_test": 150,
        "n_neg_test": 150,
        "n_val": 200,
        "n_pos_val": 100,
        "n_neg_val": 100
      },
      "threshold": Infinity,
      "threshold_source": "validation",
      "auroc@test": 0.5110222222222223,
      "auprc@test": 0.5072889303278763,
      "accuracy@thr": 0.5,
      "precision@thr": 0.0,
      "recall@thr": 0.0,
      "f1@thr": 0.0,
      "eer@test": 0.4733333333333334,
      "tpr@fpr=0.010@test": 0.0,
      "selection_details": {
        "eer_sel": 0.5800000000000001,
        "thr_eer_sel": -0.09756861701607704
      }
    }
  },
  "KGW_baseline": {
    "file": "data/KGW_paraphrase.json",
    "algorithm": "KGW",
    "language": null,
    "quality": {
      "paraphrase": {
        "sacrebleu": {
          "score": 21.561987833166636,
          "sys_len": 59305,
          "ref_len": 129214
        },
        "rouge": {
          "rouge1": {
            "p": 0.8317405453022052,
            "r": 0.44305233643486897,
            "f1": 0.5286345465582276
          },
          "rouge2": {
            "p": 0.7107054299450202,
            "r": 0.4043535619062105,
            "f1": 0.47526263650034817
          },
          "rougeL": {
            "p": 0.7500235257126517,
            "r": 0.420129565162993,
            "f1": 0.49519479889202284
          }
        }
      }
    }
  },
  "XSIR_baseline": {
    "file": "data/XSIR_paraphrase.json",
    "algorithm": "XSIR",
    "language": null,
    "quality": {
      "paraphrase": {
        "sacrebleu": {
          "score": 17.59028097494795,
          "sys_len": 60509,
          "ref_len": 143874
        },
        "rouge": {
          "rouge1": {
            "p": 0.8256459805656571,
            "r": 0.39273572524631817,
            "f1": 0.4862005006791784
          },
          "rouge2": {
            "p": 0.6983812205144662,
            "r": 0.3551203190892981,
            "f1": 0.43226782025144317
          },
          "rougeL": {
            "p": 0.7372099998378151,
            "r": 0.3690175652981322,
            "f1": 0.4509485140220981
          }
        }
      }
    }
  },
  "Unigram_baseline": {
    "file": "data/Unigram_paraphrase.json",
    "algorithm": "Unigram",
    "language": null,
    "quality": {
      "paraphrase": {
        "sacrebleu": {
          "score": 15.273252785385843,
          "sys_len": 57953,
          "ref_len": 144448
        },
        "rouge": {
          "rouge1": {
            "p": 0.8271767752272952,
            "r": 0.3686644757881753,
            "f1": 0.4686055789105386
          },
          "rouge2": {
            "p": 0.6972425779233251,
            "r": 0.3275178414588323,
            "f1": 0.4103206130139279
          },
          "rougeL": {
            "p": 0.7348433481031718,
            "r": 0.3422677972234806,
            "f1": 0.429965859720141
          }
        }
      }
    }
  }
}