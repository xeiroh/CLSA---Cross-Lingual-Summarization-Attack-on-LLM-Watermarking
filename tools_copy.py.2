# minimal example sketch (adapt paths/models to your GPU/CPU)
import os, glob
import tokenize
from xml.parsers.expat import model
from datasets.arrow_dataset import Dataset
from datasets.dataset_dict import DatasetDict, IterableDatasetDict
from datasets.iterable_dataset import IterableDataset
import torch
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM  # support both
from transformers import LogitsProcessorList
from markllm.watermark.auto_watermark import AutoWatermark, WATERMARK_MAPPING_NAMES
from markllm.utils.transformers_config import TransformersConfig
import warnings
import os as _os
import numpy as np
from functools import lru_cache
from pipeline import translate

SIR_MAPPING_DIR = os.path.join("workspace", "CLSA---Cross-Lingual-Summarization-Attack-on-LLM-Watermarking", ".venv", "lib", "python3.12", "site-packages", "markllm", "watermark", "sir", "mapping")  # Adjust as needed

warnings.filterwarnings("ignore")  # transformers logging
# Avoid transformers using torch.inference_mode inside generate (causes XSIR/SIR issues)
_os.environ.setdefault("TRANSFORMERS_NO_INFERENCE_MODE", "1")
_os.environ.setdefault("TOKENIZERS_PARALLELISM", "false")

# device = "mps" if torch.backends.mps.is_available() else "cpu"
device = "cuda" if torch.cuda.is_available() else "cpu"
torch.backends.cuda.matmul.allow_tf32 = True
torch.set_float32_matmul_precision("high")
print("Using device:", device, "- cuda available:", torch.cuda.is_available(), "- cuda version:", torch.version.cuda)
RESULTS_PATH = os.path.join(os.path.dirname(__file__), "..", "data", "results")

def load_model(model_name: str | None = None, algorithm: str = "KGW", max_tokens: int = 256):
	"""Load Mistral 7B and a MarkLLM watermark model.

	Parameters:
	- model_name: Optional HF id. Defaults to Mistral 7B.
	- algorithm: MarkLLM algorithm name (e.g., 'KGW', 'Unigram', 'SIR').
	- max_tokens: forwarded to MarkLLM gen kwargs.

	Returns: (tokenizer, gen_model, wm_model)
	"""	
	# 1) Load tokenizer/model (Mistral 7B Base by default)
	model_name = model_name or "mistralai/Mistral-7B-v0.1"
	print(f"[models] Loading tokenizer: {model_name}")
	tok = AutoTokenizer.from_pretrained(model_name, use_fast=False, trust_remote_code=True)
	
	
	torch_dtype = torch.float16 if device == "cuda" else torch.float32
	print(f"[models] Loading model: {model_name} (torch_dtype={torch_dtype}, device_map=auto)")
	try:
		gen_model = AutoModelForCausalLM.from_pretrained(
			model_name,
			torch_dtype=torch_dtype,
			device_map="auto"
		)
	except TypeError:
		# Fallback for older transformers versions
		gen_model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)
		try:
			gen_model.to(device)
		except Exception:
			pass
	try:
		if len(tok) != gen_model.get_input_embeddings().weight.shape[0]:
			print(f"[warning] Vocab size mismatch: tokenizer {len(tok)} vs model {gen_model.get_input_embeddings().weight.shape[0]}. Resizing model embeddings.")
			gen_model.resize_token_embeddings(len(tok))
	except Exception:
		pass
	# Ensure PAD token id is set
	try:
		gen_model.config.pad_token_id = tok.pad_token_id
	except Exception:
		pass
	# Disable cache to avoid issues with some watermark processors
	gen_model.config.use_cache = False
	if hasattr(gen_model, "generation_config") and gen_model.generation_config is not None:
		gen_model.generation_config.use_cache = False

	# 2) Build watermark model
	print(f"[watermark] Building watermark model: {algorithm}")

	
	tf_cfg = TransformersConfig(model=gen_model, tokenizer=tok, device=device, max_new_tokens=max_tokens)
	wm_model = AutoWatermark.load(algorithm_name=algorithm, transformers_config=tf_cfg)
	# If SIR is requested but mapping length mismatches vocab, fall back to KGW

	return tok, gen_model, wm_model

def load_data(language="english"):
	"""Load XLSum split, falling back to cached English if needed.

	Attempts to load the requested language config. If it isn't available
	(e.g., offline and not cached), falls back to 'english' which you appear
	to have cached already. If both fail, raises FileNotFoundError so callers
	can skip that run gracefully.
	"""
	try:
		return load_dataset("csebuetnlp/xlsum", language, split="test", download_mode="reuse_cache_if_exists")
	except Exception as e:
		fallback = "english"
		if language != fallback:
			print(f"Warning: XLSum config '{language}' not available (offline or not cached). Falling back to '{fallback}'. Error: {e}")
		try:
			return load_dataset("csebuetnlp/xlsum", fallback, split="test", download_mode="reuse_cache_if_exists")
		except Exception as e2:
			raise FileNotFoundError(f"Failed to load XLSum for '{language}' and fallback '{fallback}'. {e2}") from e

def _json_default(o):
	"""Best-effort JSON serializer for numpy/torch objects."""
	try:
		import numpy as np
		if isinstance(o, (np.integer,)):
			return int(o)
		if isinstance(o, (np.floating,)):
			return float(o)
		if isinstance(o, (np.ndarray,)):
			return o.tolist()
	except Exception:
		pass
	try:
		import torch as _torch
		if isinstance(o, _torch.Tensor):
			return o.detach().cpu().tolist()
	except Exception:
		pass
	return str(o)


def save_file(data, filename: str | None = None, out_dir: str | None = None, name: str | None = None, with_timestamp: bool = True) -> str:
	"""
	Save `data` (dict/list/str) to JSON file.

	- If `filename` is provided, writes to that path (adds .json if missing).
	  If `filename` is relative with no directory, it is placed under `RESULTS_PATH`.
	- Else writes to `{out_dir or RESULTS_PATH}`/`{name or 'results'}_[timestamp].json`.
	- Creates parent directories as needed.
	- Returns the absolute path written.
	"""
	from pathlib import Path
	import json
	import time

	base_dir = Path(out_dir) if out_dir else Path(RESULTS_PATH)

	if filename:
		path = Path(filename)
		# If filename is relative and has no explicit parent, place under RESULTS_PATH
		if not path.is_absolute() and path.parent == Path('.'):
			path = base_dir / path
		if path.suffix == "":
			path = path.with_suffix(".json")
	else:
		base = name or "results"
		stamp = time.strftime("%Y%m%d_%H%M%S") if with_timestamp else None
		fname = f"{base}_{stamp}.json" if stamp else f"{base}.json"
		path = base_dir / fname

	# Ensure directory exists
	path.parent.mkdir(parents=True, exist_ok=True)

	# If data is not a string, dump as JSON with sensible defaults
	if isinstance(data, (dict, list)):
		with open(path, "w") as f:
			json.dump(data, f, indent=2, default=_json_default)
	else:
		# Write raw text
		with open(path, "w") as f:
			f.write(str(data))

	return str(path.resolve())


def load_file(filename: str, as_json: bool | None = None):
	"""
	Load content from a file. If `as_json` is True, parse JSON.
	If `as_json` is None, infer from file extension:
	- .json -> json.load
	- .jsonl/.ndjson -> parse each line as JSON and return a list
	- otherwise -> return raw text
	"""
	from pathlib import Path
	import json

	path = Path(filename)
	# If given a bare filename or non-existent relative path, try under RESULTS_PATH
	if not path.is_absolute() and not path.exists():
		candidate = Path(RESULTS_PATH) / path
		if candidate.exists():
			path = candidate
	if not path.exists():
		# raise FileNotFoundError(f"No such file: {path}")
		return None

	ext = path.suffix.lower()
	if as_json is True or (as_json is None and ext in {".json", ".jsonl", ".ndjson"}):
		text = path.read_text()
		if ext in {".jsonl", ".ndjson"}:
			return [json.loads(line) for line in text.splitlines() if line.strip()]
		return json.loads(text)
	else:
		return path.read_text()

def _make_prompt(text, max_chars=2000, language="english"):
	"""Build a summarization prompt without heavy translation model loads.

	Using a lightweight, hardcoded instruction per language avoids loading
	large translation models (e.g., M2M100-1.2B) just to translate a fixed
	phrase, which can cause long startup delays or OOM.
	"""
	lang = (language or "english").lower()
	instruction_map = {
		"english": "Summarize the following text:",
		"swahili": "Fupisha maandishi yafuatayo:",
		"spanish": "Resume el siguiente texto:",
		"amharic": "የሚከተለውን ጽሑፍ አጭር አድርግ:",
		# Fallback for other languages: English instruction
	}
	instr = instruction_map.get(lang, instruction_map["english"])
	return instr + "\n\n" + text[:max_chars]

def split_dataset(dataset, sample_size=100):
	sample_size = min(sample_size, len(dataset))
	idx = np.random.choice(len(dataset), size=sample_size, replace=False)
	watermark_idx = set(idx[: sample_size // 2])
	non_watermark_idx = set(idx[sample_size // 2 :])
	watermark_samples = dataset.select(list(watermark_idx))
	non_watermark_samples = dataset.select(list(non_watermark_idx))
	return watermark_samples, non_watermark_samples

def split_and_generate(model_components, dataset, language="amharic", sample_size=100, max_chars=2000):
	"""Split dataset into two halves and generate watermarked and unwatermarked outputs sequentially."""
	watermark_samples, non_watermark_samples = split_dataset(dataset, sample_size=sample_size)
	det_wm = generate(model_components, watermark_samples, watermark=True, max_chars=max_chars, language=language)
	det_uwm = generate(model_components, non_watermark_samples, watermark=False, max_chars=max_chars, language=language)
	return det_wm + det_uwm

def generate(model_components, dataset, watermark: bool, language='english', max_chars=2000):
	"""Generate outputs sequentially (no batching) to minimize VRAM use.

	- Runs single-sample generation on GPU in float16 when available.
	- Forces num_beams=1 and use_cache=False.
	- Applies watermark via logits_processor when `watermark=True`.
	"""
	prompts = [_make_prompt(item["text"], max_chars, language) for item in dataset]
	n = len(prompts)
	results: list[dict | None] = [None] * n

	tokenizer, gen_model, wm = model_components
	cfg = getattr(wm, "config", object())
	logits_proc = getattr(wm, "logits_processor", None)

	# Prepare model
	if gen_model is not None:
		gen_model.eval()
		try:
			gen_model.to(device)
		except Exception:
			pass

	try:
		max_new = int(getattr(cfg, "gen_kwargs", {}).get("max_new_tokens", 128))
	except Exception:
		max_new = 128

	# Sequential generation to keep VRAM low
	for i in tqdm(range(n), desc="Sequential generation"):
		text = prompts[i]
		enc = tokenizer(text, return_tensors="pt", padding=False, truncation=True)
		enc = {k: v.to(device) for k, v in enc.items()}

		use_lp = LogitsProcessorList([logits_proc]) if (watermark and logits_proc) else None

		with torch.no_grad():
			out = gen_model.generate(
				**enc,
				max_new_tokens=max_new,
				do_sample=True,
				logits_processor=use_lp,
				use_cache=False,
				num_beams=1,
			)

		dec = tokenizer.batch_decode(out)
		gen_text = dec[0] if dec else ""

		det = wm.detect_watermark(gen_text)
		det["generated_text"] = gen_text
		det["true_label"] = watermark
		results[i] = det

		# Free up memory between iterations
		if device == "cuda":
			try:
				torch.cuda.empty_cache()
			except Exception:
				pass

	return [r for r in results if r is not None]
	
	
	

def detect(samples, model, column='generated_text', workers=4):
	detections = []
	column = column
	def work(p):
		detect = model.detect_watermark(p)
		detections.append(detect)
	
	with ThreadPoolExecutor(max_workers=workers) as executor:
		futures = [executor.submit(work, p[column]) for p in tqdm(samples)]
		for future in tqdm(as_completed(futures), total=len(futures)):
			future.result()
	return detections
